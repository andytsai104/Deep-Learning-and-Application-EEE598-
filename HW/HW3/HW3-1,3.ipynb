{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.ndimage import shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image (1920*2880) and translate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "img_path = 'P1/testimg.jpg'\n",
    "original_img = Image.open(img_path)     # Image Class\n",
    "\n",
    "# Translate the image\n",
    "img_np = np.array(original_img)            # dtype: [[R, G, B]*2880]\n",
    "translated_img_np = shift(img_np, shift=[0, -1, 0], mode='nearest')        # shift(shift=[height, width, RGB]): shift image along each axis\n",
    "\n",
    "# Show the image\n",
    "translated_img = Image.fromarray(translated_img_np)\n",
    "translated_img.save('P1/translated_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1 (MAE): 109.51044071903935\n",
      "Loss 2 (MSE): 37.839024944540895\n"
     ]
    }
   ],
   "source": [
    "# l1: Mean Absolute Error\n",
    "l1 = np.mean(np.abs(img_np - translated_img_np))\n",
    "print(f'Loss 1 (MAE): {l1}')\n",
    "\n",
    "# l2: Mean Square Error\n",
    "l2 = np.mean(np.square(img_np - translated_img_np))\n",
    "print(f'Loss 2 (MSE): {l2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download VGG-16 and show structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Model Summary:\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 244, 244]           1,792\n",
      "              ReLU-2         [-1, 64, 244, 244]               0\n",
      "            Conv2d-3         [-1, 64, 244, 244]          36,928\n",
      "              ReLU-4         [-1, 64, 244, 244]               0\n",
      "         MaxPool2d-5         [-1, 64, 122, 122]               0\n",
      "            Conv2d-6        [-1, 128, 122, 122]          73,856\n",
      "              ReLU-7        [-1, 128, 122, 122]               0\n",
      "            Conv2d-8        [-1, 128, 122, 122]         147,584\n",
      "              ReLU-9        [-1, 128, 122, 122]               0\n",
      "        MaxPool2d-10          [-1, 128, 61, 61]               0\n",
      "           Conv2d-11          [-1, 256, 61, 61]         295,168\n",
      "             ReLU-12          [-1, 256, 61, 61]               0\n",
      "           Conv2d-13          [-1, 256, 61, 61]         590,080\n",
      "             ReLU-14          [-1, 256, 61, 61]               0\n",
      "           Conv2d-15          [-1, 256, 61, 61]         590,080\n",
      "             ReLU-16          [-1, 256, 61, 61]               0\n",
      "        MaxPool2d-17          [-1, 256, 30, 30]               0\n",
      "           Conv2d-18          [-1, 512, 30, 30]       1,180,160\n",
      "             ReLU-19          [-1, 512, 30, 30]               0\n",
      "           Conv2d-20          [-1, 512, 30, 30]       2,359,808\n",
      "             ReLU-21          [-1, 512, 30, 30]               0\n",
      "           Conv2d-22          [-1, 512, 30, 30]       2,359,808\n",
      "             ReLU-23          [-1, 512, 30, 30]               0\n",
      "        MaxPool2d-24          [-1, 512, 15, 15]               0\n",
      "           Conv2d-25          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-26          [-1, 512, 15, 15]               0\n",
      "           Conv2d-27          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-28          [-1, 512, 15, 15]               0\n",
      "           Conv2d-29          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-30          [-1, 512, 15, 15]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 258.51\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 786.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load VGG16\n",
    "model = models.vgg16(pretrained=True).to(device)\n",
    "print('VGG16 Model Summary:')\n",
    "summary = summary(model, input_size=(3, 244, 244), device=device)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change of selected features:\n",
      "Layer 4: 213.1699676513672\n",
      "Layer 9: 279.9030456542969\n",
      "Layer 12: 363.60198974609375\n",
      "Layer 30: 11.8388032913208\n"
     ]
    }
   ],
   "source": [
    "vgg16_feature = models.vgg16(pretrained=True).features.eval().to(device)\n",
    "\n",
    "# Define an image processing function\n",
    "def preprocessIMG(image):\n",
    "    img_tensor = transforms.Compose([\n",
    "        transforms.Resize(256),     # Resize the image\n",
    "        transforms.ToTensor(),      # Translate the image to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # Image Normalization\n",
    "    ])(image).unsqueeze(0)\n",
    "    return img_tensor.to(device)\n",
    "\n",
    "\n",
    "org_img_tensor = preprocessIMG(original_img)\n",
    "trans_img_tensor = preprocessIMG(translated_img)\n",
    "\n",
    "# Define an feature extraction function\n",
    "def FeatureExtract(model, image, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "\n",
    "# Pick 4 different places for extracting features\n",
    "a, b, c, d = '4', '9', '12', '30'\n",
    "layers = {a: f'Layer {a}', b: f'Layer {b}', c: f'Layer {c}', d: f'Layer {d}'}\n",
    "\n",
    "# Extract features in original image and translated image (type: dictionary)\n",
    "org_feature = FeatureExtract(vgg16_feature, org_img_tensor, layers)\n",
    "trans_feature = FeatureExtract(vgg16_feature, trans_img_tensor, layers)\n",
    "\n",
    "# Define a function that compute the change in feature space\n",
    "def Difference(org_feature, trans_feature):\n",
    "    difference = {}\n",
    "    for layer in org_feature:\n",
    "        difference[layer] = torch.norm((org_feature[layer] - trans_feature[layer]), p=2).item()\n",
    "    return difference\n",
    "\n",
    "diff = Difference(org_feature, trans_feature)\n",
    "print(\"Change of selected features:\")\n",
    "for key in diff:\n",
    "    print(f\"{key}: {diff[key]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_1 of original image and noise image: 175.97600507571374\n",
      "l_1 of translated image and noise image: 175.99028826678241\n",
      "\n",
      "l_2 of original image and noise image: 102.50470757378473\n",
      "l_2 of translated image and noise image: 102.49764756944444\n",
      "\n",
      "Perceptual loss of original image: 132.41009521484375\n",
      "Perceptual loss of translated image: 134.11668395996094\n"
     ]
    }
   ],
   "source": [
    "def add_gaussian_noise(image, mean=0, var=1000):\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    # Generate Gaussian noise\n",
    "    row, col, ch = img_array.shape\n",
    "    sigma = var ** 0.5\n",
    "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "    noisy_image = img_array + gauss.reshape(row, col, ch)\n",
    "\n",
    "    # Clip values to stay in the valid range [0, 255]\n",
    "    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "# Add noises to boh images\n",
    "noise_org = add_gaussian_noise(original_img, mean=90)\n",
    "noise_trans = add_gaussian_noise(translated_img, mean=90)\n",
    "\n",
    "# Turn image to array\n",
    "noise_org_np = np.array(noise_org)\n",
    "noise_trans_np = np.array(noise_trans)\n",
    "\n",
    "# # Save noise images\n",
    "# noise_org.save('P1/noise_org.jpg')\n",
    "# noise_trans.save('P1/noise_trans.jpg')\n",
    "\n",
    "# Turn noise images to tensor\n",
    "noise_org_tensor = preprocessIMG(noise_org)\n",
    "noise_trans_tensor = preprocessIMG(noise_trans)\n",
    "\n",
    "# Extract features from noise images and original images\n",
    "num = '30'\n",
    "layer_name = 'Conv5_3'\n",
    "layer = {num: layer_name}\n",
    "org_feature = FeatureExtract(vgg16_feature, org_img_tensor, layer)\n",
    "trans_feature = FeatureExtract(vgg16_feature, trans_img_tensor, layer)\n",
    "noise_org_feature = FeatureExtract(vgg16_feature, noise_org_tensor, layer)\n",
    "noise_trans_feature = FeatureExtract(vgg16_feature, noise_trans_tensor, layer)\n",
    "\n",
    "# Calculate the perceptual loss\n",
    "diff_org = Difference(org_feature, noise_org_feature)\n",
    "diff_trans = Difference(trans_feature, noise_trans_feature)\n",
    "\n",
    "# Calculate l_1, l_2\n",
    "l1_org = np.mean(np.abs(img_np - noise_org_np))\n",
    "l1_trans = np.mean(np.abs(translated_img_np - noise_trans_np))\n",
    "l2_org = np.mean(np.square(img_np - noise_org_np))\n",
    "l2_trans = np.mean(np.square(translated_img_np - noise_trans_np))\n",
    "\n",
    "# Print out l_1, l_2\n",
    "print(f'l_1 of original image and noise image: {l1_org}')\n",
    "print(f'l_1 of translated image and noise image: {l1_trans}', end='\\n\\n')\n",
    "print(f'l_2 of original image and noise image: {l2_org}')\n",
    "print(f'l_2 of translated image and noise image: {l2_trans}', end='\\n\\n')\n",
    "\n",
    "# Print out perceptual loss\n",
    "print(f'Perceptual loss of original image: {diff_org[layer_name]}')\n",
    "print(f'Perceptual loss of translated image: {diff_trans[layer_name]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prototype datasetfrom ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading imagefolder...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Path to ImageNet training set\n",
    "path = '/data/datasets/community/deeplearning/imagenet/train'\n",
    "# Load ImageNet training set\n",
    "imagenet_data = datasets.ImageFolder(root=path, transform=transform)\n",
    "# imagenet_data\n",
    "print('Finished Loading imagefolder...')\n",
    "\n",
    "# Sub-sample classes (e.g., 10 classes)\n",
    "selected_classes = random.sample(range(1000), 10)  # Randomly select 10 classes\n",
    "\n",
    "# Collect indices for samples belonging to selected classes\n",
    "indices = [i for i, (_, class_idx) in enumerate(imagenet_data) if class_idx in selected_classes]\n",
    "\n",
    "# Create a subset dataset for prototyping\n",
    "subsampled_data = Subset(imagenet_data, indices)\n",
    "\n",
    "# Split into train, val, and test sets\n",
    "train_size = int(0.7 * len(subsampled_data))\n",
    "val_size = int(0.15 * len(subsampled_data))\n",
    "test_size = len(subsampled_data) - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(subsampled_data, [train_size, val_size, test_size])\n",
    "\n",
    "# Loaders (for visualization and later model training)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Visualizing class distribution using PIL\n",
    "def visualize_class_distribution_pil(loader, title):\n",
    "    # Calculate class counts\n",
    "    class_counts = torch.zeros(10)\n",
    "    for _, labels in loader:\n",
    "        for label in labels:\n",
    "            class_counts[label] += 1\n",
    "\n",
    "    # Create a blank image for the chart\n",
    "    width, height = 400, 300\n",
    "    image = Image.new('RGB', (width, height), 'white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Set up bar chart parameters\n",
    "    bar_width = 30\n",
    "    spacing = 10\n",
    "    max_height = 150  # Max height for the tallest bar\n",
    "\n",
    "    # Normalize class counts for visualization\n",
    "    max_count = class_counts.max().item()\n",
    "    class_counts = class_counts / max_count * max_height\n",
    "\n",
    "    # Draw bars\n",
    "    for i, count in enumerate(class_counts):\n",
    "        x1 = i * (bar_width + spacing) + 50\n",
    "        y1 = height - 50 - count\n",
    "        x2 = x1 + bar_width\n",
    "        y2 = height - 50\n",
    "        draw.rectangle([x1, y1, x2, y2], fill=\"blue\")\n",
    "\n",
    "        # Add class labels\n",
    "        draw.text((x1 + bar_width // 4, height - 40), str(i), fill=\"black\")\n",
    "\n",
    "    # Add title and labels\n",
    "    draw.text((width // 2 - 40, 10), title, fill=\"black\")\n",
    "    draw.text((width // 2 - 40, height - 20), 'Classes', fill=\"black\")\n",
    "    draw.text((10, height // 2), 'Number of Images', fill=\"black\")\n",
    "\n",
    "    # Show the image\n",
    "    image.show()\n",
    "\n",
    "# Visualize the class distribution using PIL\n",
    "visualize_class_distribution_pil(train_loader, 'Train Set Distribution')\n",
    "visualize_class_distribution_pil(val_loader, 'Validation Set Distribution')\n",
    "visualize_class_distribution_pil(test_loader, 'Test Set Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design 36-layer Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomResNet36(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Skip connection for matching dimensions\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define ResNet Structure\n",
    "class CustomResNet36(nn.Module):  # Renamed class to avoid conflict\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(CustomResNet36, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial layer before residual blocks\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 4 layers with residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Function that build 36-layer Resnet\n",
    "def ResNet36():\n",
    "    return CustomResNet36(ResidualBlock, [4, 4, 4, 4])\n",
    "\n",
    "net = ResNet36()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
